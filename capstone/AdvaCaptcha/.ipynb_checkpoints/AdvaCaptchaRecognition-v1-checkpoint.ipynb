{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "import imutils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting Part\n",
    "\n",
    "\n",
    "> at this level i'm working on a small set of the sample\n",
    "- convert  image to grayscale \n",
    "- split the image at specific coordinate x,y (no contour) \n",
    "- save each charcter as a single image (no cleanup)\n",
    "## extracting letters by splitting only (no cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing image 1/214\n",
      "[INFO] processing image 2/214\n",
      "[INFO] processing image 3/214\n",
      "[INFO] processing image 4/214\n",
      "[INFO] processing image 5/214\n",
      "[INFO] processing image 6/214\n",
      "[INFO] processing image 7/214\n",
      "[INFO] processing image 8/214\n",
      "[INFO] processing image 9/214\n",
      "[INFO] processing image 10/214\n",
      "[INFO] processing image 11/214\n",
      "[INFO] processing image 12/214\n",
      "[INFO] processing image 13/214\n",
      "[INFO] processing image 14/214\n",
      "[INFO] processing image 15/214\n",
      "[INFO] processing image 16/214\n",
      "[INFO] processing image 17/214\n",
      "[INFO] processing image 18/214\n",
      "[INFO] processing image 19/214\n",
      "[INFO] processing image 20/214\n",
      "[INFO] processing image 21/214\n",
      "[INFO] processing image 22/214\n",
      "[INFO] processing image 23/214\n",
      "[INFO] processing image 24/214\n",
      "[INFO] processing image 25/214\n",
      "[INFO] processing image 26/214\n",
      "[INFO] processing image 27/214\n",
      "[INFO] processing image 28/214\n",
      "[INFO] processing image 29/214\n",
      "[INFO] processing image 30/214\n",
      "[INFO] processing image 31/214\n",
      "[INFO] processing image 32/214\n",
      "[INFO] processing image 33/214\n",
      "[INFO] processing image 34/214\n",
      "[INFO] processing image 35/214\n",
      "[INFO] processing image 36/214\n",
      "[INFO] processing image 37/214\n",
      "[INFO] processing image 38/214\n",
      "[INFO] processing image 39/214\n",
      "[INFO] processing image 40/214\n",
      "[INFO] processing image 41/214\n",
      "[INFO] processing image 42/214\n",
      "[INFO] processing image 43/214\n",
      "[INFO] processing image 44/214\n",
      "[INFO] processing image 45/214\n",
      "[INFO] processing image 46/214\n",
      "[INFO] processing image 47/214\n",
      "[INFO] processing image 48/214\n",
      "[INFO] processing image 49/214\n",
      "[INFO] processing image 50/214\n",
      "[INFO] processing image 51/214\n",
      "[INFO] processing image 52/214\n",
      "[INFO] processing image 53/214\n",
      "[INFO] processing image 54/214\n",
      "[INFO] processing image 55/214\n",
      "[INFO] processing image 56/214\n",
      "[INFO] processing image 57/214\n",
      "[INFO] processing image 58/214\n",
      "[INFO] processing image 59/214\n",
      "[INFO] processing image 60/214\n",
      "[INFO] processing image 61/214\n",
      "[INFO] processing image 62/214\n",
      "[INFO] processing image 63/214\n",
      "[INFO] processing image 64/214\n",
      "[INFO] processing image 65/214\n",
      "[INFO] processing image 66/214\n",
      "[INFO] processing image 67/214\n",
      "[INFO] processing image 68/214\n",
      "[INFO] processing image 69/214\n",
      "[INFO] processing image 70/214\n",
      "[INFO] processing image 71/214\n",
      "[INFO] processing image 72/214\n",
      "[INFO] processing image 73/214\n",
      "[INFO] processing image 74/214\n",
      "[INFO] processing image 75/214\n",
      "[INFO] processing image 76/214\n",
      "[INFO] processing image 77/214\n",
      "[INFO] processing image 78/214\n",
      "[INFO] processing image 79/214\n",
      "[INFO] processing image 80/214\n",
      "[INFO] processing image 81/214\n",
      "[INFO] processing image 82/214\n",
      "[INFO] processing image 83/214\n",
      "[INFO] processing image 84/214\n",
      "[INFO] processing image 85/214\n",
      "[INFO] processing image 86/214\n",
      "[INFO] processing image 87/214\n",
      "[INFO] processing image 88/214\n",
      "[INFO] processing image 89/214\n",
      "[INFO] processing image 90/214\n",
      "[INFO] processing image 91/214\n",
      "[INFO] processing image 92/214\n",
      "[INFO] processing image 93/214\n",
      "[INFO] processing image 94/214\n",
      "[INFO] processing image 95/214\n",
      "[INFO] processing image 96/214\n",
      "[INFO] processing image 97/214\n",
      "[INFO] processing image 98/214\n",
      "[INFO] processing image 99/214\n",
      "[INFO] processing image 100/214\n",
      "[INFO] processing image 101/214\n",
      "[INFO] processing image 102/214\n",
      "[INFO] processing image 103/214\n",
      "[INFO] processing image 104/214\n",
      "[INFO] processing image 105/214\n",
      "[INFO] processing image 106/214\n",
      "[INFO] processing image 107/214\n",
      "[INFO] processing image 108/214\n",
      "[INFO] processing image 109/214\n",
      "[INFO] processing image 110/214\n",
      "[INFO] processing image 111/214\n",
      "[INFO] processing image 112/214\n",
      "[INFO] processing image 113/214\n",
      "[INFO] processing image 114/214\n",
      "[INFO] processing image 115/214\n",
      "[INFO] processing image 116/214\n",
      "[INFO] processing image 117/214\n",
      "[INFO] processing image 118/214\n",
      "[INFO] processing image 119/214\n",
      "[INFO] processing image 120/214\n",
      "[INFO] processing image 121/214\n",
      "[INFO] processing image 122/214\n",
      "[INFO] processing image 123/214\n",
      "[INFO] processing image 124/214\n",
      "[INFO] processing image 125/214\n",
      "[INFO] processing image 126/214\n",
      "[INFO] processing image 127/214\n",
      "[INFO] processing image 128/214\n",
      "[INFO] processing image 129/214\n",
      "[INFO] processing image 130/214\n",
      "[INFO] processing image 131/214\n",
      "[INFO] processing image 132/214\n",
      "[INFO] processing image 133/214\n",
      "[INFO] processing image 134/214\n",
      "[INFO] processing image 135/214\n",
      "[INFO] processing image 136/214\n",
      "[INFO] processing image 137/214\n",
      "[INFO] processing image 138/214\n",
      "[INFO] processing image 139/214\n",
      "[INFO] processing image 140/214\n",
      "[INFO] processing image 141/214\n",
      "[INFO] processing image 142/214\n",
      "[INFO] processing image 143/214\n",
      "[INFO] processing image 144/214\n",
      "[INFO] processing image 145/214\n",
      "[INFO] processing image 146/214\n",
      "[INFO] processing image 147/214\n",
      "[INFO] processing image 148/214\n",
      "[INFO] processing image 149/214\n",
      "[INFO] processing image 150/214\n",
      "[INFO] processing image 151/214\n",
      "[INFO] processing image 152/214\n",
      "[INFO] processing image 153/214\n",
      "[INFO] processing image 154/214\n",
      "[INFO] processing image 155/214\n",
      "[INFO] processing image 156/214\n",
      "[INFO] processing image 157/214\n",
      "[INFO] processing image 158/214\n",
      "[INFO] processing image 159/214\n",
      "[INFO] processing image 160/214\n",
      "[INFO] processing image 161/214\n",
      "[INFO] processing image 162/214\n",
      "[INFO] processing image 163/214\n",
      "[INFO] processing image 164/214\n",
      "[INFO] processing image 165/214\n",
      "[INFO] processing image 166/214\n",
      "[INFO] processing image 167/214\n",
      "[INFO] processing image 168/214\n",
      "[INFO] processing image 169/214\n",
      "[INFO] processing image 170/214\n",
      "[INFO] processing image 171/214\n",
      "[INFO] processing image 172/214\n",
      "[INFO] processing image 173/214\n",
      "[INFO] processing image 174/214\n",
      "[INFO] processing image 175/214\n",
      "[INFO] processing image 176/214\n",
      "[INFO] processing image 177/214\n",
      "[INFO] processing image 178/214\n",
      "[INFO] processing image 179/214\n",
      "[INFO] processing image 180/214\n",
      "[INFO] processing image 181/214\n",
      "[INFO] processing image 182/214\n",
      "[INFO] processing image 183/214\n",
      "[INFO] processing image 184/214\n",
      "[INFO] processing image 185/214\n",
      "[INFO] processing image 186/214\n",
      "[INFO] processing image 187/214\n",
      "[INFO] processing image 188/214\n",
      "[INFO] processing image 189/214\n",
      "[INFO] processing image 190/214\n",
      "[INFO] processing image 191/214\n",
      "[INFO] processing image 192/214\n",
      "[INFO] processing image 193/214\n",
      "[INFO] processing image 194/214\n",
      "[INFO] processing image 195/214\n",
      "[INFO] processing image 196/214\n",
      "[INFO] processing image 197/214\n",
      "[INFO] processing image 198/214\n",
      "[INFO] processing image 199/214\n",
      "[INFO] processing image 200/214\n",
      "[INFO] processing image 201/214\n",
      "[INFO] processing image 202/214\n",
      "[INFO] processing image 203/214\n",
      "[INFO] processing image 204/214\n",
      "[INFO] processing image 205/214\n",
      "[INFO] processing image 206/214\n",
      "[INFO] processing image 207/214\n",
      "[INFO] processing image 208/214\n",
      "[INFO] processing image 209/214\n",
      "[INFO] processing image 210/214\n",
      "[INFO] processing image 211/214\n",
      "[INFO] processing image 212/214\n",
      "[INFO] processing image 213/214\n",
      "[INFO] processing image 214/214\n"
     ]
    }
   ],
   "source": [
    "CAPTCHA_IMAGE_FOLDER = \"training_set\"\n",
    "OUTPUT_FOLDER = \"simple_output\"\n",
    "\n",
    "# Get a list of all the captcha images we need to process\n",
    "captcha_image_files = glob.glob(os.path.join(CAPTCHA_IMAGE_FOLDER, \"*\"))\n",
    "counts = {}\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, captcha_image_file) in enumerate(captcha_image_files):\n",
    "    print(\"[INFO] processing image {}/{}\".format(i + 1, len(captcha_image_files)))\n",
    "\n",
    "    # Since the filename contains the captcha text (i.e. \"2A2X.png\" has the text \"2A2X\"),\n",
    "    # grab the base filename as the text\n",
    "    filename = os.path.basename(captcha_image_file)\n",
    "    captcha_correct_text = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(captcha_image_file)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    first = thresh[0:,27:50]\n",
    "    second = thresh[0:,49:72]\n",
    "    third = thresh[0:,69:92]\n",
    "    forth = thresh[0:,92:115]\n",
    "    fivth = thresh[0:,115:138]\n",
    "\n",
    "    images = [first, second, third, forth, fivth]\n",
    "    \n",
    "\n",
    "    for letter_bounding_box, letter_text in zip(images, captcha_correct_text):\n",
    "\n",
    "        # Get the folder to save the image in\n",
    "        save_path = os.path.join(OUTPUT_FOLDER, letter_text)\n",
    "\n",
    "        # if the output directory does not exist, create it\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        # write the letter image to a file\n",
    "        count = counts.get(letter_text, 1)\n",
    "        p = os.path.join(save_path, \"{}.jpg\".format(str(count).zfill(6)))\n",
    "        cv2.imwrite(p, letter_bounding_box)\n",
    "\n",
    "        # increment the count for the current key\n",
    "        counts[letter_text] = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Part\n",
    "\n",
    "> **TODO:**\n",
    "- resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_IMAGES_FOLDER = 'simple_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img =[]\n",
    "labels = []\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER):\n",
    "    image = cv2.imread(image_file,cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    #image = resiz(image, 20, 20)\n",
    "    \n",
    "    # Adding a third channel dimension\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "    \n",
    "    # Grabbing the name of the letter \n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "    \n",
    "    # appending\n",
    "    img.append(image)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(img)\n",
    "y = np.array(labels)\n",
    "\n",
    "#scale the raw pixel intensities to the range [0, 1] to improve training\n",
    "X = np.array(X, dtype=\"float\") / 255.0\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)\n",
    "\n",
    "# encoding categorical data (get dummy variables)\n",
    "lb = LabelBinarizer().fit(y_train)\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "\n",
    "# 2 converatail layers\n",
    "cnn_model.add(Conv2D(20, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "cnn_model.add(Conv2D(50, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# flatt layer\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "#fully connected layer\n",
    "cnn_model.add(Dense(500, activation=\"relu\"))\n",
    "\n",
    "\n",
    "#output layer\n",
    "cnn_model.add(Dense(19, activation=\"softmax\"))\n",
    "\n",
    "cnn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 749 samples, validate on 321 samples\n",
      "Epoch 1/30\n",
      "749/749 [==============================] - 4s 6ms/step - loss: 2.5508 - accuracy: 0.2483 - val_loss: 1.7004 - val_accuracy: 0.5389\n",
      "Epoch 2/30\n",
      "749/749 [==============================] - 3s 3ms/step - loss: 1.2541 - accuracy: 0.6368 - val_loss: 1.2320 - val_accuracy: 0.6511\n",
      "Epoch 3/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.8046 - accuracy: 0.7744 - val_loss: 1.1004 - val_accuracy: 0.7165\n",
      "Epoch 4/30\n",
      "749/749 [==============================] - 3s 3ms/step - loss: 0.5879 - accuracy: 0.8371 - val_loss: 0.8919 - val_accuracy: 0.7414\n",
      "Epoch 5/30\n",
      "749/749 [==============================] - 3s 3ms/step - loss: 0.3726 - accuracy: 0.8919 - val_loss: 0.8887 - val_accuracy: 0.7757\n",
      "Epoch 6/30\n",
      "749/749 [==============================] - 3s 3ms/step - loss: 0.2442 - accuracy: 0.9266 - val_loss: 0.7941 - val_accuracy: 0.7757\n",
      "Epoch 7/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.1312 - accuracy: 0.9680 - val_loss: 1.0548 - val_accuracy: 0.7477\n",
      "Epoch 8/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.1331 - accuracy: 0.9680 - val_loss: 0.8786 - val_accuracy: 0.8037\n",
      "Epoch 9/30\n",
      "749/749 [==============================] - 3s 4ms/step - loss: 0.1022 - accuracy: 0.9626 - val_loss: 0.9502 - val_accuracy: 0.7726\n",
      "Epoch 10/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0562 - accuracy: 0.9800 - val_loss: 0.9045 - val_accuracy: 0.8069\n",
      "Epoch 11/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0302 - accuracy: 0.9907 - val_loss: 0.8879 - val_accuracy: 0.8162\n",
      "Epoch 12/30\n",
      "749/749 [==============================] - 3s 4ms/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 1.0721 - val_accuracy: 0.7975\n",
      "Epoch 13/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0245 - accuracy: 0.9933 - val_loss: 0.9280 - val_accuracy: 0.8037\n",
      "Epoch 14/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0090 - accuracy: 0.9987 - val_loss: 1.0472 - val_accuracy: 0.8131\n",
      "Epoch 15/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.0267 - val_accuracy: 0.8037\n",
      "Epoch 16/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.0986 - val_accuracy: 0.8162\n",
      "Epoch 17/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.1047 - val_accuracy: 0.8069\n",
      "Epoch 18/30\n",
      "749/749 [==============================] - 3s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.1273 - val_accuracy: 0.8100\n",
      "Epoch 19/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.1520 - val_accuracy: 0.8162\n",
      "Epoch 20/30\n",
      "749/749 [==============================] - 3s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1643 - val_accuracy: 0.8131\n",
      "Epoch 21/30\n",
      "749/749 [==============================] - 3s 4ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.1827 - val_accuracy: 0.8131\n",
      "Epoch 22/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 9.0461e-04 - accuracy: 1.0000 - val_loss: 1.1998 - val_accuracy: 0.8100\n",
      "Epoch 23/30\n",
      "749/749 [==============================] - 3s 4ms/step - loss: 8.6296e-04 - accuracy: 1.0000 - val_loss: 1.2098 - val_accuracy: 0.8131\n",
      "Epoch 24/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 7.2185e-04 - accuracy: 1.0000 - val_loss: 1.2369 - val_accuracy: 0.8131\n",
      "Epoch 25/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 6.6668e-04 - accuracy: 1.0000 - val_loss: 1.2365 - val_accuracy: 0.8131\n",
      "Epoch 26/30\n",
      "749/749 [==============================] - 3s 3ms/step - loss: 6.0460e-04 - accuracy: 1.0000 - val_loss: 1.2513 - val_accuracy: 0.8131\n",
      "Epoch 27/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 5.6489e-04 - accuracy: 1.0000 - val_loss: 1.2630 - val_accuracy: 0.8131\n",
      "Epoch 28/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 5.2364e-04 - accuracy: 1.0000 - val_loss: 1.2736 - val_accuracy: 0.8131\n",
      "Epoch 29/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 4.9574e-04 - accuracy: 1.0000 - val_loss: 1.2827 - val_accuracy: 0.8131\n",
      "Epoch 30/30\n",
      "749/749 [==============================] - 2s 3ms/step - loss: 4.6399e-04 - accuracy: 1.0000 - val_loss: 1.2906 - val_accuracy: 0.8131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a44cb60b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting \n",
    "cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.290605925324335\n",
      "Test accuracy: 0.8130841255187988\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
